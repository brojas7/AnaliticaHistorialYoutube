{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ff0ef39",
      "metadata": {
        "id": "8ff0ef39"
      },
      "source": [
        "\n",
        "# **Cuaderno de Proyecto — Ciencia de Datos con YouTube**\n",
        "**Curso:** SINT-200  \n",
        "**Profesor:** Dr. Tomás de Camino Beck  \n",
        "**Estudiante(s):** _Bernal Rojas Villalobos_  \n",
        "**Fecha de entrega:** 21 de Octubre\n",
        "\n",
        "---\n",
        "\n",
        "## Instrucciones Generales\n",
        "\n",
        "Reto: Exportar datos de tu actividad YouTube (Google Takeout), construir una matriz usuario-contenido con señales (vistas, likes, tiempo, etc.), hacer EDA de sesgos/“burbujas”, y entrenar dos recomendadores (colaborativo vs. basado en contenido). Comparar métricas (cosas como precision@k, recall@k, cobertura) y explicar errores.  \n",
        "\n",
        "\n",
        "\n",
        "Este cuaderno sirve como **especificación y entregable** del proyecto. Debes completar cada sección marcada con **TODO** y dejar celdas de código **ejecutables** y **reproducibles**. El reto tiene dos proyectos:\n",
        "\n",
        "1. **Proyecto 1 — Tu Huella YouTube: Recomendador y Análisis de Burbuja Algorítmica.**  \n",
        "2. **Proyecto 2 — Detección de “Doomscrolling”: Predicción de sesiones extendidas.**\n",
        "\n",
        "### Ética y Privacidad de Datos\n",
        "- Puedes **anonimizar** tu información de YouTube (IDs, títulos, canales, tiempos) antes de subirla aquí.  \n",
        "- Alternativamente, puedes usar datos de otra persona **con su consentimiento informado** y **anonimizados**.  \n",
        "- No incluyas PII (información personal identificable) ni material sensible.  \n",
        "- Incluye un **Anexo de Privacidad** explicando qué datos usaste, cómo los obtuviste y cómo los protegiste.\n",
        "\n",
        "### Entregables\n",
        "- Este **cuaderno de Colab** completo y ejecutable.  \n",
        "- Carpeta `data/` con **muestras** de los datos (o datos sintéticos/anonimizados).  \n",
        "- **Diccionario de datos** (descripción de campos, tipos, unidades, supuestos).  \n",
        "- **Resultados y visualizaciones** dentro del notebook.  \n",
        "- **Conclusiones** + **Recomendaciones** (acciones sugeridas) + **Limitaciones** + **Trabajo futuro**.\n",
        "- Repositorio con estructura mínima:  \n",
        "  ```\n",
        "  README.md\n",
        "  data/        # muestras o datos anonimizados\n",
        "  notebooks/   # este cuaderno\n",
        "  src/         # funciones reutilizables\n",
        "  reports/     # figuras / tablas clave\n",
        "  ```\n",
        "\n",
        "### Rúbrica (100 pts)\n",
        "- **Charter/Problema y utilidad (10 pts)**: objetivos claros, hipótesis, valor para el usuario.  \n",
        "- **Adquisición y calidad de datos (10 pts)**: trazabilidad, permisos, limpieza básica.  \n",
        "- **EDA y visualizaciones (20 pts)**: distribución, outliers, correlaciones, sesgos/segmentos.  \n",
        "- **Baselines y metodología (10 pts)**: definición de referencia simple y por qué.  \n",
        "- **Modelado (20 pts)**: al menos **2 enfoques** comparados, justificación.  \n",
        "- **Evaluación (15 pts)**: métricas adecuadas, validación (temporal cuando aplique), error analysis.  \n",
        "- **Reproducibilidad (5 pts)**: semillas, funciones, estructura clara.  \n",
        "- **Conclusiones & ética (10 pts)**: hallazgos accionables y reflexiones de privacidad/sesgo.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b975eb",
      "metadata": {
        "id": "e7b975eb"
      },
      "source": [
        "\n",
        "## 0. Preparación del entorno (ejecutar una vez)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clonar tu repositorio\n",
        "!git clone https://github.com/brojas7/AnaliticaHistorialYoutube.git\n",
        "\n",
        "# Ir al directorio del proyecto\n",
        "%cd AnaliticaHistorialYoutube\n",
        "\n",
        "# TODO: Ajusta versiones si lo necesitas. Evita dependencias innecesarias.\n",
        "!pip -q install pandas numpy matplotlib scikit-learn textblob python-dateutil tqdm"
      ],
      "metadata": {
        "id": "LFe1U4XL8UJA",
        "outputId": "1e547aa5-5c25-4c0c-8b61-f893b90e8f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LFe1U4XL8UJA",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AnaliticaHistorialYoutube'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 12 (delta 1), reused 12 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (12/12), 2.33 MiB | 3.85 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "/content/AnaliticaHistorialYoutube/AnaliticaHistorialYoutube\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "54899c40",
      "metadata": {
        "id": "54899c40",
        "outputId": "cb1ecbeb-2694-4d86-8302-e850d101e4aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entorno listo. Versión de pandas: 2.2.2\n"
          ]
        }
      ],
      "source": [
        "# Imports base y configuración\n",
        "import os, json, math, random, itertools, collections, gzip, re, string, time, zipfile, io\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil import parser as dateparser\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix, mean_absolute_error, mean_squared_error\n",
        ")\n",
        "\n",
        "# Reproducibilidad\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 120)\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "\n",
        "print(\"Entorno listo. Versión de pandas:\", pd.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d2a739",
      "metadata": {
        "id": "13d2a739"
      },
      "source": [
        "\n",
        "## 1. Anexo de Privacidad y Origen de Datos (obligatorio)\n",
        "**TODO:** Explicar:\n",
        "- Fuente de datos (Google Takeout, exportación manual, datos de tercero con consentimiento, etc.).  \n",
        "- Estrategia de **anonimización** (por ejemplo: hashing de IDs/URLs, truncado de timestamps, agrupación por hora/día).  \n",
        "- Contenido eliminado o agregado (p. ej., datos sintéticos para completar campos).  \n",
        "- Limitaciones y riesgos residuales.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "191b4ac5",
      "metadata": {
        "id": "191b4ac5"
      },
      "source": [
        "\n",
        "## 2. Selección de Proyecto\n",
        "**Marca con una X**\n",
        "\n",
        "- [ ] **Proyecto 1 — Recomendador YouTube & Burbuja Algorítmica**  \n",
        "- [ ] **Proyecto 2 — Detección de Doomscrolling (clasificación temporal)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b85beb5a",
      "metadata": {
        "id": "b85beb5a"
      },
      "source": [
        "\n",
        "## 3. Utilidades comunes para YouTube (ingesta y parsing)\n",
        "\n",
        "Para **Proyecto 1** y **Proyecto 2** puedes usar datos de **Google Takeout**:  \n",
        "- `watch-history.json` (o `watch-history.html` en exportaciones antiguas)  \n",
        "- `search-history.json` (opcional)  \n",
        "- `likes.csv` / `subscriptions.csv` (según disponibilidad)\n",
        "\n",
        "> **Nota:** Los formatos de Takeout pueden cambiar con el tiempo. Ajusta el parser según tu exportación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3d494986",
      "metadata": {
        "id": "3d494986"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Utilidades de parsing (ajustar a tu Takeout) ===\n",
        "\n",
        "def load_watch_history(path):\n",
        "    \"\"\"Carga watch-history desde JSON (Takeout). Devuelve DataFrame con:\n",
        "    ['timestamp', 'title', 'channel', 'channel_id', 'video_id', 'url'].\n",
        "    \"\"\"\n",
        "    # Admite .json o .json.gz\n",
        "    if path.endswith(\".gz\"):\n",
        "        with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "    else:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "    rows = []\n",
        "    for e in data:\n",
        "        # Los campos pueden variar; ajusta las claves.\n",
        "        time_str = e.get('time') or e.get('timestamp') or e.get('creationTime')\n",
        "        ts = None\n",
        "        if time_str:\n",
        "            try:\n",
        "                ts = dateparser.parse(time_str)\n",
        "            except Exception:\n",
        "                ts = None\n",
        "\n",
        "        title = e.get('title') or e.get('header') or e.get('titleUrl') or \"\"\n",
        "        # Canal y URL pueden estar anidados en 'subtitles' o 'details'\n",
        "        channel = None\n",
        "        url = None\n",
        "\n",
        "        # Caso típico de Takeout moderno:\n",
        "        if 'titleUrl' in e:\n",
        "            url = e['titleUrl']\n",
        "        if 'subtitles' in e and isinstance(e['subtitles'], list) and e['subtitles']:\n",
        "            channel = e['subtitles'][0].get('name')\n",
        "\n",
        "        # Heurística para video_id a partir de URL\n",
        "        video_id = None\n",
        "        if url and \"watch?v=\" in url:\n",
        "            video_id = url.split(\"watch?v=\")[-1].split(\"&\")[0]\n",
        "\n",
        "        rows.append({\n",
        "            \"timestamp\": ts,\n",
        "            \"title\": title,\n",
        "            \"channel\": channel,\n",
        "            \"channel_id\": None,   # Completar si tienes otro archivo con IDs\n",
        "            \"video_id\": video_id,\n",
        "            \"url\": url\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def anonymize_df(df, cols_to_hash=(\"video_id\",\"channel\",\"url\",\"title\")):\n",
        "    \"\"\"Ejemplo simple de anonimización por hashing (no reversible).\n",
        "    Asegúrate de comprender implicaciones y colisiones.\n",
        "    \"\"\"\n",
        "    def _hash(x):\n",
        "        if pd.isna(x):\n",
        "            return x\n",
        "        return str(abs(hash(str(x))) % (10**12))\n",
        "    df = df.copy()\n",
        "    for c in cols_to_hash:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].apply(_hash)\n",
        "    return df\n",
        "\n",
        "\n",
        "def sessionize(df, gap_minutes=30):\n",
        "    \"\"\"Crea sesiones a partir de eventos ordenados por tiempo.\n",
        "    gap_minutes: nuevo id de sesión cuando hay un gap > gap_minutes.\n",
        "    Agrega 'session_id' y 'session_idx' por usuario (aquí asumimos 1 usuario).\n",
        "    \"\"\"\n",
        "    df = df.sort_values(\"timestamp\").reset_index(drop=True).copy()\n",
        "    session_ids = []\n",
        "    current_sid = 0\n",
        "    prev_ts = None\n",
        "    for i, row in df.iterrows():\n",
        "        ts = row['timestamp']\n",
        "        if prev_ts is None or (ts - prev_ts).total_seconds() > gap_minutes*60:\n",
        "            current_sid += 1\n",
        "        session_ids.append(current_sid)\n",
        "        prev_ts = ts\n",
        "    df['session_id'] = session_ids\n",
        "    df['session_idx'] = df.groupby('session_id').cumcount()\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d53a88c",
      "metadata": {
        "id": "1d53a88c"
      },
      "source": [
        "\n",
        "# **Proyecto 1 — Tu Huella YouTube: Recomendador & Burbuja Algorítmica**\n",
        "\n",
        "### Objetivo\n",
        "1) Construir **dos recomendadores** con tus datos de visualización:  \n",
        "   - **Baseline de popularidad** (o popularidad por canal/categoría).  \n",
        "   - **Modelo basado en contenido** (TF‑IDF/embeddings por título/canal) **o** **colaborativo** (si tienes interacciones de múltiples usuarios/fuentes).  \n",
        "2) Medir **Precision@k, Recall@k y Coverage** (y *diversidad*) en un esquema **offline**.  \n",
        "3) Analizar posibles **sesgos o “burbujas”** (temas/canales dominantes por hora, día, duración).  \n",
        "\n",
        "### Requisitos mínimos\n",
        "- **EDA**: distribución de vistas por canal, hora del día, día de semana, duración de sesiones, *top‑k* temas.  \n",
        "- **Ingeniería de features** (ej.: tokenización títulos, lematización opcional, normalización de canales).  \n",
        "- **Comparación de al menos 2 enfoques** de recomendación.  \n",
        "- **Evaluación offline** con *train/test split temporal*.  \n",
        "- **Análisis de errores** y discusión de sesgos/limitaciones.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Charter del Proyecto 1 (llenar)\n",
        "**TODO:** Define el propósito, preguntas clave y utilidad (qué decisiones permitirán tus hallazgos).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38774f14",
      "metadata": {
        "id": "38774f14"
      },
      "source": [
        "\n",
        "## 5. Carga de datos (Proyecto 1)\n",
        "**TODO:** Sube tu `watch-history.json` (anonimizado si aplica) a `data/` y cárgalo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ec29c085",
      "metadata": {
        "id": "ec29c085",
        "outputId": "66569407-2aa6-427d-a90b-0781ba568eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         timestamp                      title channel  \\\n",
            "0 2018-04-14 03:50:32.842000+00:00           Buscaste bella c    None   \n",
            "1 2018-04-14 03:50:35.809000+00:00          Buscaste bella ci    None   \n",
            "2 2018-04-14 03:50:38.384000+00:00         Buscaste bella cio    None   \n",
            "3 2018-04-14 03:50:41.624000+00:00        Buscaste bella cio     None   \n",
            "4 2018-04-14 16:27:00.660000+00:00  Buscaste sobredosis romeo    None   \n",
            "\n",
            "  channel_id video_id                                                url  \n",
            "0       None     None  https://www.youtube.com/results?search_query=b...  \n",
            "1       None     None  https://www.youtube.com/results?search_query=b...  \n",
            "2       None     None  https://www.youtube.com/results?search_query=b...  \n",
            "3       None     None  https://www.youtube.com/results?search_query=b...  \n",
            "4       None     None  https://www.youtube.com/results?search_query=s...  \n",
            "Eventos: 12090 Rango: 2018-04-14 03:50:32.842000+00:00 -> 2025-10-18 01:02:50.991000+00:00\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ejemplo de carga (ajusta la ruta)\n",
        "PATH_WATCH = \"data/historial-de-búsqueda.json\"  # TODO: cambia si es necesario\n",
        "df_watch = load_watch_history(PATH_WATCH)\n",
        "print(df_watch.head())\n",
        "print(\"Eventos:\", len(df_watch), \"Rango:\", df_watch['timestamp'].min(), \"->\", df_watch['timestamp'].max())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e32a093",
      "metadata": {
        "id": "7e32a093"
      },
      "source": [
        "\n",
        "## 6. EDA (Proyecto 1)\n",
        "**TODO:** Explora sesgos por canal/tema/horario. Muestra tablas y gráficos clave.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6b0474",
      "metadata": {
        "id": "1c6b0474"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: EDA — ejemplos\n",
        "# Conteos por canal\n",
        "if 'channel' in df_watch.columns:\n",
        "    print(df_watch['channel'].value_counts().head(15))\n",
        "\n",
        "# Distribución por hora (si hay timestamps)\n",
        "df_watch['hour'] = df_watch['timestamp'].dt.hour\n",
        "print(df_watch['hour'].value_counts().sort_index())\n",
        "\n",
        "# (Agrega visualizaciones y tablas adicionales)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c69ef9",
      "metadata": {
        "id": "35c69ef9"
      },
      "source": [
        "\n",
        "## 7. Partición temporal y definición de tareas (Proyecto 1)\n",
        "**TODO:** Define ventana de entrenamiento y de evaluación para recomendación **offline**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c0a861",
      "metadata": {
        "id": "a4c0a861"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Split temporal: por ejemplo, último 20% del tiempo como test\n",
        "cut_ts = df_watch['timestamp'].quantile(0.8)\n",
        "train = df_watch[df_watch['timestamp'] <= cut_ts].copy()\n",
        "test  = df_watch[df_watch['timestamp'] >  cut_ts].copy()\n",
        "\n",
        "print(\"train:\", train['timestamp'].min(), \"->\", train['timestamp'].max(), \"n=\", len(train))\n",
        "print(\"test :\", test['timestamp'].min(),  \"->\", test['timestamp'].max(),  \"n=\", len(test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcf711a3",
      "metadata": {
        "id": "fcf711a3"
      },
      "source": [
        "\n",
        "## 8. Baseline de popularidad (Proyecto 1)\n",
        "Genera recomendaciones **sin personalización** como referencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b18472",
      "metadata": {
        "id": "a2b18472"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Top-N por popularidad (baseline)\n",
        "K = 10  # tamaño de recomendación\n",
        "top_items = train['video_id'].value_counts().head(100).index.tolist()\n",
        "\n",
        "def recommend_popularity(k=K):\n",
        "    return top_items[:k]\n",
        "\n",
        "# Conjunto de ítems verdaderos en test (lo visto en test)\n",
        "true_items = set(test['video_id'].dropna().unique().tolist())\n",
        "\n",
        "def precision_at_k(recommended, true_set):\n",
        "    if len(recommended) == 0: return 0.0\n",
        "    hit = sum(1 for x in recommended if x in true_set)\n",
        "    return hit / len(recommended)\n",
        "\n",
        "def recall_at_k(recommended, true_set):\n",
        "    if len(true_set) == 0: return 0.0\n",
        "    hit = sum(1 for x in recommended if x in true_set)\n",
        "    return hit / len(true_set)\n",
        "\n",
        "# Eval baseline\n",
        "rec = recommend_popularity(K)\n",
        "p = precision_at_k(rec, true_items)\n",
        "r = recall_at_k(rec, true_items)\n",
        "coverage = len(set(top_items)) / max(1, df_watch['video_id'].nunique())\n",
        "\n",
        "print(f\"Baseline Popularidad -> P@{K}={p:.3f}  R@{K}={r:.3f}  Cobertura={coverage:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f75a6b",
      "metadata": {
        "id": "97f75a6b"
      },
      "source": [
        "\n",
        "## 9. Recomendador basado en contenido **(ejemplo TF-IDF por título/canal)**\n",
        "**TODO:** Implementa TF‑IDF (o embeddings) y calcula similitud contenido‑a‑contenido para recomendar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23706ddb",
      "metadata": {
        "id": "23706ddb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# EJEMPLO ESQUELETO (completa con TF-IDF real si lo deseas)\n",
        "# Aquí usamos una heurística mínima por canal/tokens de título para ilustrar el flujo.\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize_title(s):\n",
        "    if pd.isna(s): return []\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"[^a-z0-9áéíóúüñ\\s]\", \" \", s)\n",
        "    tok = [t for t in s.split() if len(t) > 2]\n",
        "    return tok\n",
        "\n",
        "train = train.copy()\n",
        "train['tokens'] = train['title'].apply(tokenize_title)\n",
        "\n",
        "# \"Perfil\" de intereses por tokens (muy básico)\n",
        "profile = Counter(itertools.chain.from_iterable(train['tokens'].tolist()))\n",
        "top_terms = [t for t, _ in profile.most_common(50)]\n",
        "\n",
        "def recommend_content_based(k=10):\n",
        "    # Recomienda ítems del set de entrenamiento por coincidencia con términos del perfil\n",
        "    scores = []\n",
        "    for vid, grp in train.groupby('video_id'):\n",
        "        toks = list(itertools.chain.from_iterable(grp['tokens'].tolist()))\n",
        "        score = sum(1 for t in toks if t in top_terms)\n",
        "        scores.append((vid, score))\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [vid for vid, s in scores[:k]]\n",
        "\n",
        "rec_cb = recommend_content_based(K)\n",
        "p_cb = precision_at_k(rec_cb, true_items)\n",
        "r_cb = recall_at_k(rec_cb, true_items)\n",
        "cov_cb = len(set(train['video_id'])) / max(1, df_watch['video_id'].nunique())\n",
        "print(f\"Contenido (heurístico) -> P@{K}={p_cb:.3f}  R@{K}={r_cb:.3f}  Cobertura={cov_cb:.3f}\")\n",
        "\n",
        "# TODO: Sustituir por TF-IDF/embeddings reales y mejorar evaluación (por usuario/ventanas).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e38b1533",
      "metadata": {
        "id": "e38b1533"
      },
      "source": [
        "\n",
        "## 10. Análisis de burbuja/sesgo (Proyecto 1)\n",
        "**TODO:** Mide concentración por canal/tema, horarios de consumo, diversidad de recomendaciones.\n",
        "Propón **intervenciones** para aumentar diversidad sin perder pertinencia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fee1328",
      "metadata": {
        "id": "2fee1328"
      },
      "source": [
        "\n",
        "## 11. Conclusiones y trabajo futuro (Proyecto 1)\n",
        "**TODO:** Resume hallazgos, limitaciones y siguientes pasos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b71cd1e",
      "metadata": {
        "id": "2b71cd1e"
      },
      "source": [
        "\n",
        "# **Proyecto 2 — Detección de “Doomscrolling” (Clasificación Temporal)**\n",
        "\n",
        "### Objetivo\n",
        "Predecir si una **sesión** de consumo (YouTube u otra plataforma) terminará en **scroll extendido** o **consumo prolongado** (> X minutos o > Y videos).\n",
        "\n",
        "### Requisitos mínimos\n",
        "- **Definición de etiqueta** (doomscroll = 1 si supera umbral de tiempo o conteo).  \n",
        "- **Sessionization** con ventana de inactividad (p. ej., 30 minutos).  \n",
        "- **Features temporales** (hora/día, ritmos, gaps), contextuales (tema/canal) y de **acumulación**.  \n",
        "- **Validación temporal** (e.g., TimeSeriesSplit) para evitar *leakage*.  \n",
        "- **Comparación de al menos 2 clasificadores** (baseline + modelo).  \n",
        "- **Métricas**: precisión, recall, F1, ROC‑AUC, matriz de confusión por segmento.  \n",
        "- **Error analysis** e importancia de variables.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Charter del Proyecto 2 (llenar)\n",
        "**TODO:** Propósito, hipótesis y utilidad (p. ej., alertas o cortes saludables de uso).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ddce208",
      "metadata": {
        "id": "8ddce208"
      },
      "source": [
        "\n",
        "## 13. Carga de datos (Proyecto 2)\n",
        "**TODO:** Reutiliza `watch-history.json` u otra fuente y genera sesiones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d8929a",
      "metadata": {
        "id": "c8d8929a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ejemplo de carga y sessionization\n",
        "PATH_WATCH = \"data/watch-history.json\"  # TODO: cambia si es necesario\n",
        "df_watch = load_watch_history(PATH_WATCH)\n",
        "\n",
        "# Crea sesiones (ajusta gap si aplica)\n",
        "df_sess = sessionize(df_watch, gap_minutes=30)\n",
        "\n",
        "# Etiqueta doomscrolling: duración > umbral (estimación proxy por número de eventos)\n",
        "UMBRAL_VIDEOS = 8  # TODO: ajusta criterio (o usa tiempo real si lo tienes)\n",
        "sess_counts = df_sess.groupby('session_id').size().rename('n_events')\n",
        "df_labels = sess_counts.to_frame().assign(doom=lambda x: (x['n_events'] >= UMBRAL_VIDEOS).astype(int)).reset_index()\n",
        "\n",
        "print(df_labels['doom'].value_counts())\n",
        "df_labels.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89362440",
      "metadata": {
        "id": "89362440"
      },
      "source": [
        "\n",
        "## 14. Features y partición temporal (Proyecto 2)\n",
        "**TODO:** Crear variables por sesión (hora de inicio, día de semana, gaps promedio, temas/canales dominantes, ritmo).  \n",
        "Usa **TimeSeriesSplit** o una corte temporal para evaluación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f7bbe4",
      "metadata": {
        "id": "a7f7bbe4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Construcción simple de features por sesión (extiende según tus datos)\n",
        "def build_session_features(df_watch, df_sess):\n",
        "    # Hora de inicio de la sesión, día de semana, tamaño de sesión, ritmo aproximado\n",
        "    start_ts = df_sess.groupby('session_id')['timestamp'].min().rename('start_ts')\n",
        "    end_ts   = df_sess.groupby('session_id')['timestamp'].max().rename('end_ts')\n",
        "    n_events = df_sess.groupby('session_id').size().rename('n_events')\n",
        "\n",
        "    base = pd.concat([start_ts, end_ts, n_events], axis=1).reset_index()\n",
        "    base['duration_min'] = (base['end_ts'] - base['start_ts']).dt.total_seconds() / 60.0\n",
        "    base['hour'] = base['start_ts'].dt.hour\n",
        "    base['dow'] = base['start_ts'].dt.dayofweek  # 0=Lunes\n",
        "    base['events_per_min'] = base['n_events'] / base['duration_min'].replace(0, np.nan)\n",
        "\n",
        "    # (Opcional) top canal por sesión\n",
        "    top_channel = (\n",
        "        df_sess.groupby(['session_id','channel']).size()\n",
        "        .reset_index(name='cnt')\n",
        "        .sort_values(['session_id','cnt'], ascending=[True, False])\n",
        "        .drop_duplicates('session_id')\n",
        "        .set_index('session_id')['channel']\n",
        "        .rename('top_channel')\n",
        "    )\n",
        "    base = base.merge(top_channel, left_on='session_id', right_index=True, how='left')\n",
        "\n",
        "    # Codificación simple de canal principal (dummy)\n",
        "    if 'top_channel' in base.columns:\n",
        "        dummies = pd.get_dummies(base['top_channel'], prefix='ch', dummy_na=True)\n",
        "        base = pd.concat([base.drop(columns=['top_channel']), dummies], axis=1)\n",
        "\n",
        "    return base\n",
        "\n",
        "df_feat = build_session_features(df_watch, df_sess)\n",
        "df_all = df_feat.merge(df_labels, on='session_id', how='inner')\n",
        "\n",
        "# Partición temporal (último 20% del tiempo como test)\n",
        "cut_ts = df_all['start_ts'].quantile(0.8)\n",
        "train = df_all[df_all['start_ts'] <= cut_ts].copy()\n",
        "test  = df_all[df_all['start_ts'] >  cut_ts].copy()\n",
        "\n",
        "y_train = train['doom'].values\n",
        "y_test  = test['doom'].values\n",
        "\n",
        "X_train = train.drop(columns=['doom','start_ts','end_ts'])\n",
        "X_test  = test.drop(columns=['doom','start_ts','end_ts'])\n",
        "\n",
        "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bab5798",
      "metadata": {
        "id": "5bab5798"
      },
      "source": [
        "\n",
        "## 15. Baseline y Modelo(s) (Proyecto 2)\n",
        "**TODO:** Compara un baseline (mayoría/clasificador trivial) con al menos **un modelo** (p. ej., Árboles/GBM/RegLog).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0083a6b8",
      "metadata": {
        "id": "0083a6b8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Baseline: clase mayoritaria\n",
        "from collections import Counter\n",
        "majority = Counter(y_train).most_common(1)[0][0]\n",
        "y_pred_base = np.full_like(y_test, fill_value=majority)\n",
        "print(\"Baseline (mayoría) -> acc=%.3f  prec=%.3f  rec=%.3f  f1=%.3f\" % (\n",
        "    accuracy_score(y_test, y_pred_base),\n",
        "    precision_score(y_test, y_pred_base, zero_division=0),\n",
        "    recall_score(y_test, y_pred_base, zero_division=0),\n",
        "    f1_score(y_test, y_pred_base, zero_division=0)\n",
        "))\n",
        "\n",
        "# Modelo ejemplo: Regresión Logística (puedes sustituir por otro)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(max_iter=1000, random_state=SEED)\n",
        "clf.fit(X_train.fillna(0), y_train)\n",
        "y_proba = clf.predict_proba(X_test.fillna(0))[:,1]\n",
        "y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"LogReg -> acc=%.3f  prec=%.3f  rec=%.3f  f1=%.3f  roc=%.3f\" % (\n",
        "    accuracy_score(y_test, y_pred),\n",
        "    precision_score(y_test, y_pred, zero_division=0),\n",
        "    recall_score(y_test, y_pred, zero_division=0),\n",
        "    f1_score(y_test, y_pred, zero_division=0),\n",
        "    roc_auc_score(y_test, y_proba) if len(np.unique(y_test))>1 else float('nan')\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5044e09f",
      "metadata": {
        "id": "5044e09f"
      },
      "source": [
        "\n",
        "## 16. Análisis de errores e importancia de variables (Proyecto 2)\n",
        "**TODO:** Muestra matriz de confusión, curvas y comenta falsos positivos/negativos por segmento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41095552",
      "metadata": {
        "id": "41095552"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Matriz de confusión (numérica)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a29c3b17",
      "metadata": {
        "id": "a29c3b17"
      },
      "source": [
        "\n",
        "## 17. Conclusiones y trabajo futuro (Proyecto 2)\n",
        "**TODO:** Resume hallazgos, utilidad práctica (alertas, límites de tiempo, recomendaciones) y próximos pasos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bae64966",
      "metadata": {
        "id": "bae64966"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 18. Limitaciones generales y reflexiones éticas\n",
        "**TODO:** Reflexiona sobre sesgos, representatividad de datos personales, efectos de recomendación, privacidad y seguridad.\n",
        "\n",
        "## 19. Checklist final\n",
        "- [ ] El cuaderno **corre desde cero** (Runtime -> Restart & Run All).  \n",
        "- [ ] Incluye **diccionario de datos** y **anexo de privacidad**.  \n",
        "- [ ] Tiene **baselines** y **≥2 modelos** comparados (según el proyecto).  \n",
        "- [ ] Reporta **métricas** adecuadas y **análisis de errores**.  \n",
        "- [ ] Conclusiones accionables y **trabajo futuro**.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}